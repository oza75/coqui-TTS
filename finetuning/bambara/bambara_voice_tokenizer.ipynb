{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install transformers[torch] datasets tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import normalizers\n",
    "from typing import List\n",
    "import re\n",
    "import tokenizers\n",
    "import json\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:22:56.412178Z",
     "start_time": "2024-04-09T10:22:56.368310800Z"
    }
   },
   "id": "d0ee46fa715e90ee",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'source_dataset'],\n    num_rows: 353926\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bam_ds = load_dataset(\"oza75/bambara-texts\", split=\"train\")\n",
    "bam_ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:52:01.299522900Z",
     "start_time": "2024-04-09T00:51:58.471984700Z"
    }
   },
   "id": "d4173c29d286a63",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class VoiceBambaraTextPreprocessor:\n",
    "\n",
    "    def preprocess_batch(self, texts: List[str]) -> List[str]:\n",
    "        return [self.preprocess(text) for text in texts]\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = self.expand_number(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def expand_number(self, text):\n",
    "        \"\"\"\n",
    "        Normalize Bambara text for TTS by replacing numerical figures with their word equivalents.\n",
    "\n",
    "        Args:\n",
    "        text (str): The text to be normalized.\n",
    "    \n",
    "        Returns:\n",
    "        str: The normalized Bambara text.\n",
    "        \"\"\"\n",
    "\n",
    "        # A regex pattern to match all numbers\n",
    "        number_pattern = re.compile(r'\\b\\d+\\b')\n",
    "\n",
    "        # Function to replace each number with its Bambara text\n",
    "        def replace_number_with_text(match):\n",
    "            number = int(match.group())\n",
    "            return self.number_to_bambara(number)\n",
    "\n",
    "        # Replace each number in the text with its Bambara word equivalent\n",
    "        normalized_text = number_pattern.sub(replace_number_with_text, text)\n",
    "\n",
    "        return normalized_text\n",
    "\n",
    "    def number_to_bambara(self, n):\n",
    "\n",
    "        \"\"\"\n",
    "        Convert a number into its textual representation in Bambara using recursion.\n",
    "        Args:\n",
    "        n (int): The number to be converted.\n",
    "        Returns:\n",
    "        str: The number expressed in Bambara text.\n",
    "        Examples:\n",
    "        >>> number_to_bambara(123)\n",
    "        'kɛmɛ ni mugan ni saba'\n",
    "        Notes:\n",
    "        This function assumes that 'n' is a non-negative integer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Bambara numbering rules\n",
    "        units = [\"\", \"kɛlɛn\", \"fila\", \"saba\", \"naani\", \"duuru\", \"wɔrɔ\", \"wòlonwula\", \"sɛɛgin\", \"kɔnɔntɔn\"]\n",
    "        tens = [\"\", \"tan\", \"mugan\", \"bisaba\", \"binaani\", \"biduuru\", \"biwɔrɔ\", \"biwòlonfila\", \"bisɛɛgin\", \"bikɔnɔntɔn\"]\n",
    "        hundreds = [\"\", \"kɛmɛ\"]\n",
    "        thousands = [\"\", \"waga\"]\n",
    "        millions = [\"\", \"milyɔn\"]\n",
    "\n",
    "        # Handle zero explicitly\n",
    "        if n == 0:\n",
    "            return \"\"  # bambara does not support zero\n",
    "\n",
    "        if n < 10:\n",
    "            return units[n]\n",
    "        elif n < 100:\n",
    "            return tens[n // 10] + (\" ni \" + self.number_to_bambara(n % 10) if n % 10 > 0 else \"\")\n",
    "        elif n < 1000:\n",
    "            return hundreds[1] + (\" \" + self.number_to_bambara(n // 100) if n >= 200 else \"\") + (\" ni \" + self.number_to_bambara(n % 100) if n % 100 > 0 else \"\")\n",
    "        elif n < 1_000_000:\n",
    "            return thousands[1] + \" \" + self.number_to_bambara(n // 1000) + (\n",
    "                \" ni \" + self.number_to_bambara(n % 1000) if n % 1000 > 0 else \"\")\n",
    "        else:\n",
    "            return millions[1] + \" \" + self.number_to_bambara(n // 1_000_000) + (\n",
    "                \" ni \" + self.number_to_bambara(n % 1_000_000) if n % 1_000_000 > 0 else \"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:52:02.922221500Z",
     "start_time": "2024-04-09T00:52:02.912230600Z"
    }
   },
   "id": "4d522c52c87647d1",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=2000,\n",
    "    special_tokens=[\"[STOP]\", \"[UNK]\", \"[SPACE]\", \"[START]\", \"[bm]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "text_preprocessor = VoiceBambaraTextPreprocessor()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:41.286834700Z",
     "start_time": "2024-04-09T10:27:41.280662Z"
    }
   },
   "id": "f026ed92078ecc40",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(bam_ds), batch_size):\n",
    "        yield text_preprocessor.preprocess_batch(bam_ds[i: i + batch_size][\"text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:45.078568200Z",
     "start_time": "2024-04-09T10:27:45.075470700Z"
    }
   },
   "id": "2027bfa5070209f3",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(bam_ds))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:27:59.113969Z",
     "start_time": "2024-04-09T10:27:47.660547900Z"
    }
   },
   "id": "a730cdbc1e9162c4",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.save(\"./saved/bam_vocab.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:28:00.654940Z",
     "start_time": "2024-04-09T10:28:00.591076600Z"
    }
   },
   "id": "ec65ae1f96591fa7",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=4, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=18, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokenizer.encode_batch(text_preprocessor.preprocess_batch(bam_ds['text'][:10]))\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:28:08.772023100Z",
     "start_time": "2024-04-09T10:28:07.759462800Z"
    }
   },
   "id": "67cc3955ce86b2e0",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['bi',\n ',',\n 's',\n 'ur',\n 'ɔ',\n 'fana',\n 'dun',\n 'nen',\n 'kɔ',\n ',',\n 'n',\n 'bɛ',\n 'na',\n 'an',\n 'ka',\n 'baro',\n 'kɛ',\n '.']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[5].tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:28:10.958286900Z",
     "start_time": "2024-04-09T10:28:10.876130Z"
    }
   },
   "id": "81d3d747361b7b0e",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8425c2b611e69a52"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['ɔ',\n '̀',\n 'ɔ',\n '̀',\n 'wɔ',\n '́',\n ',',\n 'n',\n 'í',\n 'dɔ',\n '́',\n 'b',\n 'ó',\n 'lo',\n 'k',\n 'ò',\n 'ra',\n ',',\n 'n',\n 'ù',\n 'mu',\n 'kɛ',\n 'b',\n \"'\",\n 'à',\n 'fɔ',\n '́',\n 'k',\n 'ó',\n ',',\n 'dɔ',\n '́',\n 'ka',\n 'n',\n 'à']"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokenizer.encode(text_preprocessor.preprocess(\"Ɔ̀Ɔ̀ wɔ́, ní dɔ́ bólokòra, nùmukɛ b'à fɔ́ kó, dɔ́ ka nà\"))\n",
    "outputs.tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:28:16.448429800Z",
     "start_time": "2024-04-09T10:28:16.386015800Z"
    }
   },
   "id": "d00181372d83aa3f",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def integrate_vocabs(main_vocab_path, bam_vocab_path, output_dir):\n",
    "    # Load the main vocabulary\n",
    "    with open(main_vocab_path, 'r', encoding='utf-8') as f:\n",
    "        main_vocab = json.load(f)\n",
    "    main_tokens = set(main_vocab['model']['vocab'].keys())\n",
    "    next_id = max(main_vocab['model']['vocab'].values()) + 1\n",
    "\n",
    "    # Load the Bambara vocabulary\n",
    "    with open(bam_vocab_path, 'r', encoding='utf-8') as f:\n",
    "        bam_vocab = json.load(f)\n",
    "    bam_tokens = set(bam_vocab['model']['vocab'].keys())\n",
    "    \n",
    "    # Add tokens from bam_vocab to main_vocab if they don't exist\n",
    "    for token in bam_tokens:\n",
    "        if token not in main_tokens:\n",
    "            main_vocab['model']['vocab'][token] = next_id\n",
    "            next_id += 1\n",
    "    \n",
    "    # Now for the merges\n",
    "    main_merges = set(main_vocab['model']['merges'])\n",
    "    bam_merges = set(bam_vocab['model']['merges'])\n",
    "\n",
    "    # Add merges from bam_vocab to main_vocab if they don't exist\n",
    "    for merge in bam_merges:\n",
    "        if merge not in main_merges:\n",
    "            main_vocab['model']['merges'].append(merge)\n",
    "\n",
    "    # Save the updated vocabulary\n",
    "    output_vocab_path = os.path.join(output_dir, 'combined_vocab.json')\n",
    "    with open(output_vocab_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(main_vocab, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Updated vocabulary saved to {output_vocab_path}\")\n",
    "    return output_vocab_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:29:16.177405200Z",
     "start_time": "2024-04-09T10:29:16.128536900Z"
    }
   },
   "id": "464e0ac06e6bafd9",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocabulary saved to ./saved/combined_vocab.json\n"
     ]
    }
   ],
   "source": [
    "# Specify the paths to your main and Bambara vocab files\n",
    "main_vocab_path = './saved/xtts_default_vocab.json'\n",
    "bam_vocab_path = './saved/bam_vocab.json'\n",
    "output_dir = './saved'\n",
    "\n",
    "# Integrate the Bambara vocab into the main vocab and save the updated vocab\n",
    "updated_vocab_path = integrate_vocabs(main_vocab_path, bam_vocab_path, output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:29:19.620043300Z",
     "start_time": "2024-04-09T10:29:19.540517Z"
    }
   },
   "id": "23e164a6bdf63f75",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "combined_tokenizer = Tokenizer.from_file(\"./saved/combined_vocab.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:30:39.166551900Z",
     "start_time": "2024-04-09T10:30:39.130175300Z"
    }
   },
   "id": "2434350effaeabdb",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[5773,\n 41,\n 5760,\n 2839,\n 1127,\n 7738,\n 6047,\n 6888,\n 467,\n 5765,\n 59,\n 2778,\n 691,\n 14,\n 6878,\n 15,\n 1969,\n 43,\n 456,\n 27,\n 941,\n 1270,\n 7312,\n 494,\n 1153,\n 7289,\n 832,\n 512,\n 650,\n 14,\n 165,\n 2351,\n 43,\n 182,\n 571,\n 7496,\n 7798,\n 9]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_tokenizer.encode(\"Nin Avrili kalo daminɛ na Farafinna tilebiyanfan jamana dɔw la futɛni barika bonyan fo ka dama tɛmɛ.\").ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T10:32:52.032969500Z",
     "start_time": "2024-04-09T10:32:51.972958700Z"
    }
   },
   "id": "99bb3daf9aed9256",
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "12016f158baa5dbd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
