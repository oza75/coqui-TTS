{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install transformers[torch] datasets tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers import normalizers\n",
    "from typing import List\n",
    "import re\n",
    "import tokenizers"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:51:58.466157500Z",
     "start_time": "2024-04-09T00:51:57.016087700Z"
    }
   },
   "id": "d0ee46fa715e90ee",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'source_dataset'],\n    num_rows: 353926\n})"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bam_ds = load_dataset(\"oza75/bambara-texts\", split=\"train\")\n",
    "bam_ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:52:01.299522900Z",
     "start_time": "2024-04-09T00:51:58.471984700Z"
    }
   },
   "id": "d4173c29d286a63",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class VoiceBambaraTextPreprocessor:\n",
    "\n",
    "    def preprocess_batch(self, texts: List[str]) -> List[str]:\n",
    "        return [self.preprocess(text) for text in texts]\n",
    "\n",
    "    def preprocess(self, text: str) -> str:\n",
    "        text = text.lower()\n",
    "        text = self.expand_number(text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def expand_number(self, text):\n",
    "        \"\"\"\n",
    "        Normalize Bambara text for TTS by replacing numerical figures with their word equivalents.\n",
    "\n",
    "        Args:\n",
    "        text (str): The text to be normalized.\n",
    "    \n",
    "        Returns:\n",
    "        str: The normalized Bambara text.\n",
    "        \"\"\"\n",
    "\n",
    "        # A regex pattern to match all numbers\n",
    "        number_pattern = re.compile(r'\\b\\d+\\b')\n",
    "\n",
    "        # Function to replace each number with its Bambara text\n",
    "        def replace_number_with_text(match):\n",
    "            number = int(match.group())\n",
    "            return self.number_to_bambara(number)\n",
    "\n",
    "        # Replace each number in the text with its Bambara word equivalent\n",
    "        normalized_text = number_pattern.sub(replace_number_with_text, text)\n",
    "\n",
    "        return normalized_text\n",
    "\n",
    "    def number_to_bambara(self, n):\n",
    "\n",
    "        \"\"\"\n",
    "        Convert a number into its textual representation in Bambara using recursion.\n",
    "        Args:\n",
    "        n (int): The number to be converted.\n",
    "        Returns:\n",
    "        str: The number expressed in Bambara text.\n",
    "        Examples:\n",
    "        >>> number_to_bambara(123)\n",
    "        'kɛmɛ ni mugan ni saba'\n",
    "        Notes:\n",
    "        This function assumes that 'n' is a non-negative integer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Bambara numbering rules\n",
    "        units = [\"\", \"kɛlɛn\", \"fila\", \"saba\", \"naani\", \"duuru\", \"wɔrɔ\", \"wòlonwula\", \"sɛɛgin\", \"kɔnɔntɔn\"]\n",
    "        tens = [\"\", \"tan\", \"mugan\", \"bisaba\", \"binaani\", \"biduuru\", \"biwɔrɔ\", \"biwòlonfila\", \"bisɛɛgin\", \"bikɔnɔntɔn\"]\n",
    "        hundreds = [\"\", \"kɛmɛ\"]\n",
    "        thousands = [\"\", \"waga\"]\n",
    "        millions = [\"\", \"milyɔn\"]\n",
    "\n",
    "        # Handle zero explicitly\n",
    "        if n == 0:\n",
    "            return \"\"  # bambara does not support zero\n",
    "\n",
    "        if n < 10:\n",
    "            return units[n]\n",
    "        elif n < 100:\n",
    "            return tens[n // 10] + (\" ni \" + self.number_to_bambara(n % 10) if n % 10 > 0 else \"\")\n",
    "        elif n < 1000:\n",
    "            return hundreds[1] + (\" \" + self.number_to_bambara(n // 100) if n >= 200 else \"\") + (\" ni \" + self.number_to_bambara(n % 100) if n % 100 > 0 else \"\")\n",
    "        elif n < 1_000_000:\n",
    "            return thousands[1] + \" \" + self.number_to_bambara(n // 1000) + (\n",
    "                \" ni \" + self.number_to_bambara(n % 1000) if n % 1000 > 0 else \"\")\n",
    "        else:\n",
    "            return millions[1] + \" \" + self.number_to_bambara(n // 1_000_000) + (\n",
    "                \" ni \" + self.number_to_bambara(n % 1_000_000) if n % 1_000_000 > 0 else \"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:52:02.922221500Z",
     "start_time": "2024-04-09T00:52:02.912230600Z"
    }
   },
   "id": "4d522c52c87647d1",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=6681,\n",
    "    special_tokens=[\"[STOP]\", \"[UNK]\", \"[SPACE]\", \"[START]\", \"[bm]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "\n",
    "text_preprocessor = VoiceBambaraTextPreprocessor()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:52:13.602079200Z",
     "start_time": "2024-04-09T00:52:13.555312900Z"
    }
   },
   "id": "f026ed92078ecc40",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def batch_iterator(batch_size=1000):\n",
    "    for i in range(0, len(bam_ds), batch_size):\n",
    "        yield text_preprocessor.preprocess_batch(bam_ds[i: i + batch_size][\"text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:52:25.191716800Z",
     "start_time": "2024-04-09T00:52:25.177987900Z"
    }
   },
   "id": "2027bfa5070209f3",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(bam_ds))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:52:40.793575800Z",
     "start_time": "2024-04-09T00:52:26.850368900Z"
    }
   },
   "id": "a730cdbc1e9162c4",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer.save(\"./saved/vocab.json\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:52:41.977266800Z",
     "start_time": "2024-04-09T00:52:41.926781800Z"
    }
   },
   "id": "ec65ae1f96591fa7",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=2, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=9, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=8, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokenizer.encode_batch(text_preprocessor.preprocess_batch(bam_ds['text'][:10]))\n",
    "outputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:52:50.291277500Z",
     "start_time": "2024-04-09T00:52:50.240961200Z"
    }
   },
   "id": "67cc3955ce86b2e0",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['bi',\n ',',\n 'surɔ',\n 'fana',\n 'dun',\n 'nen',\n 'kɔ',\n ',',\n 'n',\n 'bɛ',\n 'na',\n 'an',\n 'ka',\n 'baro',\n 'kɛ',\n '.']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[5].tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T00:53:04.016823400Z",
     "start_time": "2024-04-09T00:53:03.975900900Z"
    }
   },
   "id": "81d3d747361b7b0e",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8425c2b611e69a52"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['ɔ̀',\n 'ɔ̀',\n 'wɔ́',\n ',',\n 'n',\n 'í',\n 'dɔ́',\n 'b',\n 'ó',\n 'lo',\n 'k',\n 'ò',\n 'ra',\n ',',\n 'n',\n 'ù',\n 'mu',\n 'kɛ',\n 'b',\n \"'\",\n 'à',\n 'fɔ́',\n 'k',\n 'ó',\n ',',\n 'dɔ́',\n 'ka',\n 'n',\n 'à']"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = tokenizer.encode(text_preprocessor.preprocess(\"Ɔ̀Ɔ̀ wɔ́, ní dɔ́ bólokòra, nùmukɛ b'à fɔ́ kó, dɔ́ ka nà\"))\n",
    "outputs.tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T05:47:58.837524800Z",
     "start_time": "2024-04-08T05:47:58.782974500Z"
    }
   },
   "id": "d00181372d83aa3f",
   "execution_count": 148
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[14017,\n 14017,\n 6414,\n 19,\n 59,\n 101,\n 4058,\n 47,\n 106,\n 387,\n 56,\n 105,\n 369,\n 19,\n 59,\n 111,\n 451,\n 368,\n 47,\n 14,\n 89,\n 3224,\n 56,\n 106,\n 19,\n 4058,\n 361,\n 59,\n 89]"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-08T05:48:03.498523400Z",
     "start_time": "2024-04-08T05:48:03.446062600Z"
    }
   },
   "id": "23e164a6bdf63f75",
   "execution_count": 149
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
