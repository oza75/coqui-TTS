{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install tokenizers datasets transformers[torch] huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset, DatasetDict\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T15:55:08.587011Z",
     "start_time": "2024-04-07T15:55:08.557678600Z"
    }
   },
   "id": "50bb787c3bb5404f",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def concatenate_text_columns(examples, text_columns):\n",
    "    \"\"\"\n",
    "    Concatenate multiple text columns into a single 'text' column.\n",
    "    \n",
    "    Parameters:\n",
    "    - examples (dict): A batch of examples from the dataset.\n",
    "    - text_columns (list of str): List of column names to concatenate.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Updated batch of examples with a concatenated 'text' column.\n",
    "    \"\"\"\n",
    "    concatenated_text = [\". \".join([examples[col] for col in text_columns]) for _ in\n",
    "                         range(len(examples[text_columns[0]]))]\n",
    "    return {\"text\": concatenated_text}\n",
    "\n",
    "\n",
    "def load_and_concatenate_datasets(dataset_args_list):\n",
    "    \"\"\"\n",
    "    Loads multiple datasets based on a list of argument dictionaries, concatenates them into a single dataset.\n",
    "    \n",
    "    The 'text_column' in each dictionary can be a list of columns to concatenate or a function to apply to the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_args_list (list of dict): Each dict contains arguments for the `load_dataset` function.\n",
    "    \n",
    "    Returns:\n",
    "    - datasets.Dataset: A concatenated dataset with 'text' and 'source_dataset' columns.\n",
    "    \"\"\"\n",
    "    concatenated_datasets = None\n",
    "    for dataset_args in tqdm(dataset_args_list):\n",
    "        # Load the dataset with provided arguments.\n",
    "        dataset = load_dataset(**{k: v for k, v in dataset_args.items() if k != 'text_column'})\n",
    "\n",
    "        # Handle the 'text_column' specification.\n",
    "        text_column = dataset_args.get('text_column', 'text')\n",
    "\n",
    "        if isinstance(text_column, list):\n",
    "            # If 'text_column' is a list, concatenate the specified columns into a new 'text' column.\n",
    "            dataset = dataset.map(lambda examples: concatenate_text_columns(examples, text_column), batched=True)\n",
    "        elif callable(text_column):\n",
    "            # If 'text_column' is a function, apply it to the dataset.\n",
    "            dataset = dataset.map(text_column, batched=True)\n",
    "        elif text_column != 'text':\n",
    "            dataset = dataset.rename_column(text_column, 'text')\n",
    "\n",
    "        # Normalize the dataset by removing the split dimension\n",
    "        if isinstance(dataset, DatasetDict):\n",
    "            dataset = Dataset.from_dict(dataset[dataset_args.get('split')])\n",
    "\n",
    "        # Ensure the 'text' column exists after processing.\n",
    "        if 'text' not in dataset.column_names:\n",
    "            raise ValueError(\"The dataset must have a 'text' column after processing 'text_column'.\")\n",
    "        \n",
    "        dataset = dataset.select_columns(['text'])\n",
    "        \n",
    "        # Add a 'source_dataset' column.\n",
    "        dataset_name = dataset_args.get('path', 'unknown_dataset')\n",
    "        dataset = dataset.map(lambda examples: {'source_dataset': [dataset_name] * len(examples['text'])}, batched=True)\n",
    "\n",
    "        # Concatenate with the previously loaded datasets.\n",
    "        if concatenated_datasets is None:\n",
    "            concatenated_datasets = dataset\n",
    "        else:\n",
    "            concatenated_datasets = concatenate_datasets([concatenated_datasets, dataset])\n",
    "\n",
    "    return concatenated_datasets"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T16:29:22.338777Z",
     "start_time": "2024-04-07T16:29:22.307302200Z"
    }
   },
   "id": "2154ff56a5826f1a",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def belebele_text(examples):\n",
    "    # Iterate through each example in the batch and concatenate the strings as desired\n",
    "    texts = [\n",
    "        f\"{flores_passage}\\n∆ùininkali: {question}\\njaabi 1 nan: {mc_answer1}\\njaabi 2 nan: {mc_answer2}\\njaabi 3 nan: {mc_answer3}\\njaabi 4 nan: {mc_answer4}\" \n",
    "        for flores_passage, question, mc_answer1, mc_answer2, mc_answer3, mc_answer4 \n",
    "        in zip(\n",
    "            examples['flores_passage'], examples['question'], examples['mc_answer1'], \n",
    "            examples['mc_answer2'], examples['mc_answer3'], examples['mc_answer4']\n",
    "        )\n",
    "    ]\n",
    "    return {'text': texts}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T15:00:16.472465800Z",
     "start_time": "2024-04-07T15:00:16.441004100Z"
    }
   },
   "id": "2141640c652a3026",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset_list = [\n",
    "    {'path': 'oza75/bambara-tts', 'text_column': 'bambara', 'split': 'train'},\n",
    "    {'path': 'sil-ai/bloom-speech', 'name': 'bam', 'text_column': 'text', 'split': 'train', 'use_auth_token': True},\n",
    "    {'path': 'sil-ai/bloom-speech', 'name': 'bam', 'text_column': 'text', 'split': 'validation',\n",
    "     'use_auth_token': True},\n",
    "    {'path': 'sil-ai/bloom-speech', 'name': 'bam', 'text_column': 'text', 'split': 'test', 'use_auth_token': True},\n",
    "    {'path': 'wikimedia/wikipedia', 'name': '20231101.bm', 'text_column': 'text', 'split': 'train'},\n",
    "    {'path': 'facebook/belebele', 'text_column': belebele_text, 'split': 'bam_Latn'},\n",
    "    {'path': 'bigscience/xP3all', 'name': 'bm', 'text_column': 'targets', 'split': 'train'},\n",
    "    {'path': 'sil-ai/bloom-captioning', 'name': 'bam', 'text_column': 'caption', 'split': 'train', 'use_auth_token': True, 'download_mode':'force_redownload'},\n",
    "    {'path': 'sil-ai/bloom-captioning', 'name': 'bam', 'text_column': 'caption', 'split': 'validation', 'use_auth_token': True, 'download_mode':'force_redownload'},\n",
    "    {'path': 'sil-ai/bloom-captioning', 'name': 'bam', 'text_column': 'caption', 'split': 'test', 'use_auth_token': True, 'download_mode':'force_redownload'},\n",
    "]\n",
    "\n",
    "bambara_ds = load_and_concatenate_datasets(dataset_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4dc193dbf2c1cc74"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "my_dataset_list = [\n",
    "    {'path': 'oza75/mt-fr-bm-texts', 'name': 'main', 'text_column': 'bambara', 'split': 'train'},\n",
    "    {'path': 'oza75/mt-fr-bm-texts', 'name': 'transcriptions', 'text_column': 'bambara', 'split': 'train'},\n",
    "    {'path': 'oza75/mt-fr-bm-texts', 'name': 'dictionnary', 'text_column': 'bambara', 'split': 'train'},\n",
    "]\n",
    "\n",
    "my_bambara_ds = load_and_concatenate_datasets(my_dataset_list)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8426785008ba405"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b967e37f3907964d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
